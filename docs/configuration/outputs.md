---
description: >-
  This document outlines the various Output types (Streaming, Triggers and
  Archives) supported by the Edge Delta agent, and how the outputs are
  configured.
---

# Outputs

### Overview

Outputs are the mechanism that tells the Edge Delta agent which destinations to send collected & generated data such as metrics, patterns, alerts etc.

[**Streaming destinations**](#streaming-destinations) are typically centralized monitoring platforms such as Splunk, Sumo Logic, Datadog, Snowflake, New Relic, Elastic, etc.

[**Trigger destinations**](#trigger-destinations) are alerting and automation systems such as PagerDuty, Slack, ServiceNow, OpsGenie, Runbook, etc. that Edge Delta can be configured to send alerts and notifications when anomalies are detected or various conditions are met.

[**Archive destinations**](#archive-destinations) are storage solutions that Edge Delta can be configured to send compressed raw data logs periodically.

_Note:_ Output destinations can be specified per-config or at organization level. [Integrations](./processors.md) page can be used to create new integration destinations and add them to existing configs.

**Features \(data sets\)**

Features represents the data types collected/generated by the agent to be sent to streaming destinations. By default cluster, metric and edac are enabled. See below for details on types of features.

* **metric**: enables sending metrics which are generated by processors in the workflow.
* **edac**: \(Edge Delta Anomaly Context\) enables sending contextual logs that happened around an anomaly.
* **cluster**: enables sending cluster patterns & samples. Patterns are in the following format: "{cluster-pattern}, {count}"
* **log**: enables forwarding raw logs to a stream destination. Default is off.
* **topk**: topk data type enables to send top-k records that are generated by top-k processor. 
* **all**: all enables all features for stream destination but note that only supported features will be working by stream destination.

## Streaming Destinations

### Splunk

If enabled, the Splunk integration will stream analytics and insights to a Splunk HEC endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "splunk" to stream data to splunk | Yes |
| endpoint | Full Splunk HEC URI for this integration | Yes |
| token | Splunk HEC Token for this integration | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: splunk-integration
        type: splunk
        endpoint: "<protocol>://<host>:<port>/<endpoint>"
        token: "32-character GUID token"
```

Or alternatively use the org level splunk integration. In this case other fields are not needed.

```yaml
      - integration: my-org-splunk
```

**Finding the appropriate HEC URI to provide for endpoint \(Splunk Enterprise, Self-Service Splunk Cloud, Managed Splunk Cloud\):** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#Send\_data\_to\_HTTP\_Event\_Collector\_on\_Splunk\_Enterprise](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#Send_data_to_HTTP_Event_Collector_on_Splunk_Enterprise)

**Generate HEC token for a Splunk endpoint:** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#About\_Event\_Collector\_tokens](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#About_Event_Collector_tokens)

**Find the HTTP Port Number used for HEC endpoints \(Global Settings\):** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#Enable\_HTTP\_Event\_Collector](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#Enable_HTTP_Event_Collector)

### Sumo Logic

If enabled, the Sumo Logic integration will stream analytics and insights to a Sumo Logic HTTPs Endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "sumologic" to stream data to sumo logic. | Yes |
| endpoint | Full HTTPs URL for this endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
```

**Creating a new Sumo Logic HTTPs Endpoint:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

**Finding an existing Sumo Logic HTTPs Endpoint URL:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

### AWS CloudWatch

If enabled, the AWS CloudWatch integration will stream logs to a given aws region.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "cloudwatch" to stream data to AWS Cloud Watch | Yes |
| region | AWS region destination for logs | Yes |
| log\_group\_name | CloudWatch log group name | Yes |
| log\_stream\_name | CloudWatch log stream name \(either name or prefix is supported not both\) | Yes |
| log\_stream\_prefix | CloudWatch log stream prefix \(either name or prefix is supported not both\) | Yes |
| auto\_create | When necessary iam policies provided if auto\_create is set, log group and log stream will be created if not exists | No |
| allow\_label\_override | monitored container can override the default values of log group name, logs stream name and log stream prefix, by setting ed\_log\_group\_name, ed\_log\_stream\_name, ed\_log\_stream\_prefix labels | No |
| auto\_configure | only supported for ECS environments, and when provided only region configuration can be provided. Automatically create LogGroupName in the format of /ecs/task\_definition\_family and LogsStreamPrefix in the format of ecs/container\_name/task\_id | No |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| features | Features defines which data types stream to backend, it can be "log" only for Amazon Cloudwatch. | No |

```yaml
      - name: cloudwatch
        type: cloudwatch
        region: us-west-2
        log_group_name: /ecs/microservice
        log_stream_prefix: ecs
        auto_create: true
        features: log
```

* Assign below permission to taskExecutionRoleArn for putting log events into CloudWatch when auto\_create is not set

  ```yaml
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

* Assign below permission to taskExecutionRoleArn if auto\_create is set

  ```yaml
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:CreateLogStream",
            "logs:CreateLogGroup",
            "logs:DescribeLogStreams",
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

**CloudWatch log group name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogGroup.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogGroup.html) _\*\*_

**CloudWatch log stream name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogStream.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogStream.html) _\*\*_

### **Datadog**

If enabled, the Datadog integration will stream analytics and insights to your Datadog environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "datadog" to stream data to Datadog | Yes |
| api\_key | Datadog API Key | Yes |
| custom\_tags | Key-values defined in custom tags by the user are streamed to datadog for every request. | No |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: datadog-integration
        type: datadog
        api_key: "<add datadog api key>"
        custom_tags:
          "app": "transaction_manager"
          "env": "pre_prod"
          "region": "us-west-2"
```

**Create a new Datadog API Key:** [https://docs.datadoghq.com/account\_management/api-app-keys/\#add-a-key](https://docs.datadoghq.com/account_management/api-app-keys/#add-a-key)

### **New Relic**

If enabled, the New Relic integration will stream analytics and insights to your New Relic environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "newrelic" to stream data to New Relic | Yes |
| api\_key | New Relic Insert API Key | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: new-relic-integration
        type: newrelic
        api_key: "<add new relic insert api key>"
```

**Create a new New Relic Insert API Key:** [https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys\#event-insert-key](https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys#event-insert-key)

### **InfluxDB**

If enabled, the InfluxDB integration will stream analytics and insights to your InfluxDB deployment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "influxdb" to stream data to Influx DB | Yes |
| endpoint | InfluxDB endpoint | Yes |
| http\_user | InfluxDB user credentials | Yes |
| http\_password | InfluxDB password for connecting user | Yes |
| db | Specific InfluxDB database to stream data to | Yes |
| features | Features defines which data types stream to backend, it can be "metric", "edac". If you don't provide any value then it is all. | No |

```yaml
      - name: influxdb-integration
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        port: 443
        http_user: admin
        http_password: your_http_password
        db: "specific_influxdb_database"
```

### **Wavefront**

If enabled, the Wavefront integration will stream analytics and insights to your Wavefront environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "wavefront" to stream data to Wavefront | Yes |
| endpoint | Wavefront endpoint | Yes |
| token | Wavefront API token | Yes |
| features | Features defines which data types stream to backend, it can be "metric" only for Wavefront. | No |

```yaml
      - name: wavefront-integration
        type: wavefront
        endpoint: "https://{your wavefront domain}.wavefront.com/report"
        token: "<add wavefront api token>"
```

### **Scalyr**

If enabled, the Scalyr integration will stream analytics and insights to your Scalyr environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "scalyr" to stream data to Scalyr | Yes |
| endpoint | Scalyr endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log" for Scalyr. | No |

```yaml
      - name: scalyr-integration
        type: scalyr
        endpoint: "https://app.scalyr.com/api/uploadLogs?token={scalyr log access write key}"
```

### **Elastic Search**

If enabled, the Elastic Search integration will stream analytics and insights to your Elastic Search environment. Elastic index template and lifecycle creation guide can be found [here](../appendices/elastic-index). It's not mandatory but highly recommended to complete those steps in the guide to prepare your Elastic Search environment to be Edgedelta streaming target.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "elastic" to stream data to Elastic | Yes |
| index | Name of elastic index \(or index template\) where data will be streamed by edgedelta agents. Set this to 'ed-agent-log' if followed the guide above | Yes |
| cloud\_id | Cloud ID of elastic search backend | No |
| address | Address list of elastic search backend | No |
| token | Elastic Search API Key | No |
| user | Username for elastic search credentials | No |
| password | Elastic Search password for connecting user | No |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

* For the connection url, you can not provide cloud\_id and address at the same time. And you must provide at least one of them.
* For the authentication, you can not provide token and user/password at the same time. And you must provide at least one of them.

```yaml
      - name: elastic-integration
        type: elastic
        index: "index name"
        # you can provide cloud or adress list but not both at the same time
        cloud_id: "<add elasticsearch cloud_id>"
        #address:
         #- <elastic search endpoint address_1>
         #- <elastic search endpoint address_2>
        # you can provide token or user/pass for auth but not both at the same time
        token: "elastic search api key"
        #user: "elastic search username"
        #password: "elastic search password"
```

### Azure AppInsight

If enabled, the Azure AppInsight integration will stream analytics and insights to an Azure endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "azure" to stream data to Azure AppInsight | Yes |
| endpoint | Azure AppInsight endpoint. | Yes |
| api\_key | Azure AppInsight API key. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: azure-integration
        type: azure
        endpoint: https://dc.services.visualstudio.com/v2/track
        api_key: "Azure AppInsight api key" 
        features: "metric"
```

### Kafka

If enabled, the Kafka integration will stream analytics and insights to an Kafka endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "kafka" to stream data to Kafka | Yes |
| endpoint | Kafka broker addresses. | Yes |
| topic | Kafka topic name. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: kafka-integration
        type: kafka
        endpoint: https://dc.services.visualstudio.com/v2/track
        endpoint: <kafka broker address-1>,<kafka broker address-2>
        topic: topic
```

### SignalFx

If enabled, the SignalFx integration will stream analytics and insights to an SignalFx endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "signalfx" to stream data to SignalFX | Yes |
| endpoint | SignalFx endpoint. | Yes |
| token | SignalFx API token. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: signalfx-integration
        type: signalfx
        endpoint: https://ingest.us1.signalfx.com/v2
        token: "<add signalfx api token>"
        features: "metric,log"
```

## Trigger Destinations

### **Slack**

If enabled, the Slack integration will stream notifications and alerts to the specified Slack channel

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "slack" to send alerts to Slack | Yes |
| endpoint | Slack Webhook or APP endpoint URL | Yes |
| suppression\_window | A [golang duration](https://golang.org/pkg/time/#ParseDuration) string that represents the suppression window. Once agent detects an issue and notifies this slack endpoint it will suppress any new issues for this duration. Default is "20m". | No |
| suppression\_mode | Suppression mode can be "local" or "global". Default is "local" which means an individual agent suppresses an issue only if it has locally notified a similar issue in last suppresson window. When "global" mode is selected an individual agent checks with Edge Delta backend to see whether there were similar alerts from other sibling agents \(the ones sharing same tag in config\). | No |
| notify\_content | Used to customize the notification content. It supports templating. | No |

#### **Notify Content**

Notify Content is optional way to customize the notification content for slack/webhook triggers. It supports templating.  
 **Available template fields**:

* **Tag**: User defined tag to describe the environment. e.g. prod\_us\_west\_2\_cluster.
* **EDAC**: Edge Delta Anomaly Context ID.
* **Host**: Hostname of the environment where agent running on.
* **ConfigID**: Configuration ID which agent is using.
* **MetricName**: Metric name causing the anomaly.
* **Source**: Source name is the identifier of the source such as docker container id or file name.
* **SourceType**: Source type. e.g. "Docker", "system"
* **SourceAttributes**: List of source attributes.
* **Timestamp**: Timestamp when event triggered.
* **Epoch**: Timestamp in epoch format when event triggered. [epoch](https://en.wikipedia.org/wiki/Epoch)
* **CurrentValue**: Metric current value.
* **ThresholdValue**: Threshold value.
* **ThresholdDescription**: Detailed threshold description including threshold type, value, etc.
* **MatchedTerm**: A sample log message causing the anomaly event.
* **ThresholdType**: Threshold type.
* **FileGlobPath**: File global path.
* **K8sPodName**: Kubernetes pod name.
* **K8sNamespace**: Kubernetes namespace.
* **K8sControllerKind**: Kubernetes controller kind.
* **K8sContainerName**: Kubernetes container name.
* **K8sContainerImage** Kubernetes container image.
* **K8sControllerLogicalName**: Kubernetes controller logical name.
* **ECSCluster**: ECS cluster name.
* **ECSContainerName**: ECS container name.
* **ECSTaskVersion**: ECS task version/
* **ECSTaskFamily**: ECS task family.
* **DockerContainerName**: Docker container name.

  _Note:_ About templates you should read before use:

* if the value is empty the item will not be sent to slack
* the keys are sorted alphabetically before sending to slack so they will not appear in the order specified in the config

**Title**: Title text for webhook message. It can be customized with custom template fields.  
 **Disable default fields**: It is used for disabling default fields in notify message. Its value is false by default.  
 **Custom Fields**: You can extend the notification content by adding name-value pairs. You can build by using template fields given above.  
 **Advanced Content**: It provides full flexibility to define the payload in slack notification post requests.

* Advanced content overrides the other settings\(title, custom fields...\)
* Custom templates are also supported in advanced content.
* You can use block kit builder tool provided by slack [https://app.slack.com/block-kit-builder](https://app.slack.com/block-kit-builder) prior to test.

```yaml
       notify_content:
         title: "Anomaly Detected: {{.ProcessorDescription}}"
         disable_default_fields: false
         custom_fields:
           "Dashboard": "https://admin.edgedelta.com/investigation?edac={{.EDAC}}&timestamp={{.Timestamp}}"
           "Current Value": "{{.CurrentValue}}"
           "Threshold Value": "{{.ThresholdValue}}"
           "Custom Message": "{{.CurrentValue}} exceeds {{.ThresholdValue}}"
           "Built-in Threshold Description": "{{.ThresholdDescription}}"
           "Matched Term": "{{.MatchedTerm}}"
           "Threshold Type": "{{.ThresholdType}}"
           "File Path": "{{.FileGlobPath}}"
           "K8s PodName": "{{.K8sPodName}}"
           "K8s Namespace": "{{.K8sNamespace}}"
           "K8s ControllerKind": "{{.K8sControllerKind}}"
           "K8s ContainerName": "{{.K8sContainerName}}"
           "K8s ContainerImage": "{{.K8sContainerImage}}"
           "K8s ControllerLogicalName": "{{.K8sControllerLogicalName}}"
           "ECSCluster": "{{.ECSCluster}}"
           "ECSContainerName": "{{.ECSContainerName}}"
           "ECSTaskVersion": "{{.ECSTaskVersion}}"
           "ECSTaskFamily": "{{.ECSTaskFamily}}"
           "DockerContainerName": "{{.DockerContainerName}}"
           "SourceAttributes": "{{.SourceAttributes}}"
           "ConfigID": "{{.ConfigID}}"
           "EDAC": "{{.EDAC}}"
           "Epoch": "{{.Epoch}}"
           "Host": "{{.Host}}"
           "MetricName": "{{.MetricName}}"
           "Source": "{{.Source}}"
           "SourceType": "{{.SourceType}}"
           "Tag": "{{.Tag}}"
```

```yaml
       notify_content:
         title: "Anomaly Detected: {{.ProcessorDescription}}"
         advanced_content: |
           {
             "blocks": [
               {
                 "type": "section",
                 "text": {
                   "type": "mrkdwn",
                   "text": "*Raw POST Anomaly Detected: {{.ProcessorDescription}}*"
                 }
               },
               {
                 "type": "section",
                 "text": {
                   "type": "mrkdwn",
                   "text": "*MatchedTerm* {{.MatchedTerm}}\n*ConfigID* {{.ConfigID}}"
                 }
               }
             ]
           }
```

```yaml
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
        notify_content:
          title: "Anomaly Detected: {{.ProcessorDescription}}"
          disable_default_fields: false
          custom_fields:
            "Dashboard": "https://admin.edgedelta.com/investigation?edac={{.EDAC}}&timestamp={{.Timestamp}}"
            "Current Value": "{{.CurrentValue}}"
            "Threshold Value": "{{.ThresholdValue}}"
            "Custom Message": "{{.CurrentValue}} exceeds {{.ThresholdValue}}"
            "Matched Term": "{{.MatchedTerm}}"
```

**Getting started with Slack Incoming Webhooks:** [https://api.slack.com/messaging/webhooks](https://api.slack.com/messaging/webhooks)

### Examples

```yaml
outputs:
  streams:
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
  triggers:
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
```

## Archive Destinations

### **AWS S3**

If enabled, the AWS S3 integration will stream logs to an AWS S3 endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "s3" to send archived logs to AWS S3 | Yes |
| bucket | Target s3 bucket to send archived logs | Yes |
| region | The specified s3 bucket's region | Yes |
| aws\_key\_id | AWS key id that has PutObject permission to target bucket. How do I create an AWS access key? [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key) | Yes |
| aws\_sec\_key | AWS secret key id that has PutObject permission to target bucket. How do I create an AWS access key? [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key) | Yes |

```yaml
      - name: my-s3
        type: s3
        bucket: testbucket
        region: us-east-2
        aws_key_id: "<add aws key id>"
        aws_sec_key: "<add aws secure key>"
```

### **Azure Blob Storage**

If enabled, the Azure Blob Storage integration will stream logs to an Azure Blob Storage endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "blob" to send archived logs to Azure blob | Yes |
| account\_name | Account Name for the azure account. | Yes |
| account\_key | Account Key for azure account. You can visit [https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) | Yes |
| container | Container to upload. | Yes |

```yaml
      - name: my-blob
        type: blob
        account_name: "<add account name>"
        account_key: "<add account key>"
        container: testcontainer
```

### **Google Cloud Storage**

If enabled, the Google Cloud Storage integration will stream logs to an GCS endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](./processors.md). It can be referred in the config via _integration\_name_ in which case rest of the fields are not required to be set. The destination's config will be pulled from backend by the agent. When _integration\_name_ is set the _name_ is ignored and _integration\_name_'s value should be used when adding this destination to a workflow. | No |
| type | Must be set to "gcs" to send archived logs to Google Cloud Storage | Yes |
| bucket | Target gcs bucket to send archived logs. | Yes |
| hmac\_access\_key | GCS HMAC Access Key which has permissions to upload files to specified bucket. See [https://cloud.google.com/storage/docs/authentication/managing-hmackeys](https://cloud.google.com/storage/docs/authentication/managing-hmackeys) for details on how to create new keys | Yes |
| hmac\_secret | GCS HMAC secret associated with the access key specified. | Yes |

```yaml
      - name: my-gcs
        type: gcs
        bucket: ed-test-bucket
        hmac_access_key: my_hmac_access_key_123
        hmac_secret: my_hmac_secret_123
```

