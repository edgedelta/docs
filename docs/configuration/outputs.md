---
description: >-
  This document outlines the various Output types (Streaming, Triggers and
  Archives) supported by the Edge Delta agent, and how the outputs are
  configured.
---

# Outputs

### Overview

Outputs are the mechanism that tells the Edge Delta agent which destinations to send collected & generated data such as metrics, patterns, alerts etc.

[**Streaming destinations**](#streaming-destinations) are typically centralized monitoring platforms such as Splunk, Sumo Logic, Datadog, Snowflake, New Relic, Elastic, etc.

[**Trigger destinations**](#trigger-destinations) are alerting and automation systems such as PagerDuty, Slack, ServiceNow, OpsGenie, Runbook, etc. that Edge Delta can be configured to send alerts and notifications when anomalies are detected or various conditions are met.

[**Archive destinations**](#archive-destinations) are storage solutions that Edge Delta can be configured to send compressed raw data logs periodically.

_Note:_ Output destinations can be specified per-config or at organization level. [Integrations](https://admin.edgedelta.com/integrations) page can be used to create new integration destinations and add them to existing configs.

**Features \(data sets\)**

Features represents the data types collected/generated by the agent to be sent to streaming destinations. By default cluster, metric and edac are enabled. See below for details on types of features.

* **metric**: enables sending metrics which are generated by processors in the workflow.
* **edac**: \(Edge Delta Anomaly Context\) enables sending contextual logs that happened around an anomaly.
* **cluster**: enables sending cluster patterns & samples. Patterns are in the following format: "{cluster-pattern}, {count}"
* **log**: enables forwarding raw logs to a stream destination. Default is off.
* **topk**: topk data type enables to send top-k records that are generated by top-k processor.
* **all**: all enables "metric", "edac" and "cluster" features for stream destination but note that only supported features will be working by stream destination.

## Streaming Destinations

### Splunk

If enabled, the Splunk integration will stream analytics and insights to a Splunk HEC endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "splunk" to stream data to splunk | Yes |
| endpoint | Full Splunk HEC URI for this integration | Yes |
| token | Splunk HEC Token for this integration | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it will be set as "metric,edac,cluster". | No |

```yaml
      - name: splunk-integration
        type: splunk
        endpoint: "<protocol>://<host>:<port>/<endpoint>"
        token: "32-character GUID token"
        index: "main"
```

Or alternatively use the org level splunk integration. In this case other fields are not needed.

```yaml
      - integration_name: my-org-splunk
```


In case multiple instances of same splunk destination needed to route different data types to different Splunk indexes:

```yaml
- name: edac-splunk-dest
  integration_name: orgs-splunk
  features: edac
  index: edac-index
- integration_name: orgs-splunk
  name: metric-splunk-dest
  features: metric
  index: metric-index
```

#### Setting up the Splunk Integration

The process is different between Splunk Cloud and Splunk Enterprise, so it is essential to follow the appropriate instructions for your environment

There are 3 important elements to your configuration:

* Configure an HEC Token in Splunk
* Determine the Correct HEC Endpoint in Splunk
* Import the Edge Delta dashboard into Splunk

#### Configuring an HEC Token in Splunk

##### Splunk Cloud
* Create a Splunk HTTP Event Collector (HEC) and Token
* In the Splunk Web UI, navigate to Settings -> Add Data
* Click “Monitor” -> “HTTP Event Listener”
* Provide a name for the HEC in the name field -> Click “Next”
* Confirm Index information or use default index -> “Click Review”
* Click “Submit”
* Copy the Token Value displayed in the Splunk Web into the “Token” field of the Edge Delta - Splunk Streaming Destination Configuration

##### Splunk Enterprise

* Ensure HTTP Event Collector (HEC) is enabled
* In the Splunk Enterprise Web UI, navigate to Settings -> Data Inputs
* Click “HTTP Event Collector”
* Click “Global Settings”
* In the “All Tokens” toggle button, select “Enabled”
* Create a Splunk HTTP Event Collector (HEC) and Token
* In the Splunk Web UI, navigate to Settings -> Add Data
* Click “Monitor” -> “HTTP Event Collector”
* Provide a name for the HEC in the name field -> Click “Next”
* Confirm Index information or use default index -> “Click Review”
* Click “Submit”
* Copy the Token Value displayed in the Splunk Web into the “Token” field of the Edge Delta - Splunk Streaming Destination Configuration


#### Determining your HEC Endpoint

Before continuing, please ensure you have gathered the following information:

* Splunk deployment type (Enterprise, Cloud, Free Trial, etc)
* Splunk hostname (from Splunk Browser URI) 
* Input  Protocol (HTTPS is default)

##### Splunk Cloud (Cloud, Free Trial, Cloud on GCP) Format

Replace <splunk_hostname> with your organization’s hostname

* Splunk Cloud
  * URI Format: https://http-inputs-<splunk_hostname>:443/services/collector/event
* Splunk Free Trial 
  * URI Format: https://inputs.<splunk_hostname>:8088/services/collector/event
* Splunk Cloud on GCP
  * URI Format: https://http-inputs.<splunk_hostname>:443/services/collector/event

##### Splunk Enterprise

* URI Format: https://<splunk_hostname>:8088/services/collector/event


#### How to import the Edge Delta Dashboard

Import this dashboard into your Splunk environment for a real-time overview of your data from Edge Delta.

* Navigate to Search UI
* Click “Dashboards”
* Click “Create New Dashboard”
* Input Dashboard Name/Description/Permissions
* Click “Classic Dashboards” -> “Create”
* In the Edit Dashboard Screen, switch from UI to Source
* Replace existing XML with copied XML from clipboard
* Switch back to UI (instead of source)
* Click “Save”


### Sumo Logic

If enabled, the Sumo Logic integration will stream analytics and insights to a Sumo Logic HTTPs Endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "sumologic" to stream data to sumo logic. | Yes |
| endpoint | Full HTTPs URL for this endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
```

**Creating a new Sumo Logic HTTPs Endpoint:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

**Finding an existing Sumo Logic HTTPs Endpoint URL:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

### AWS CloudWatch

If enabled, the AWS CloudWatch integration will stream logs to a given aws region.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "cloudwatch" to stream data to AWS Cloud Watch | Yes |
| region | AWS region destination for logs | Yes |
| log\_group\_name | CloudWatch log group name | Yes |
| log\_stream\_name | CloudWatch log stream name \(either name or prefix is supported not both\) | Yes |
| log\_stream\_prefix | CloudWatch log stream prefix \(either name or prefix is supported not both\) | Yes |
| auto\_create | When necessary iam policies provided if auto\_create is set, log group and log stream will be created if not exists | No |
| allow\_label\_override | monitored container can override the default values of log group name, logs stream name and log stream prefix, by setting ed\_log\_group\_name, ed\_log\_stream\_name, ed\_log\_stream\_prefix labels | No |
| auto\_configure | only supported for ECS environments, and when provided only region configuration can be provided. Automatically create LogGroupName in the format of /ecs/task\_definition\_family and LogsStreamPrefix in the format of ecs/container\_name/task\_id | No |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| features | Features defines which data types stream to backend, it can be "log" only for Amazon Cloudwatch. | No |

```yaml
      - name: cloudwatch
        type: cloudwatch
        region: us-west-2
        log_group_name: /ecs/microservice
        log_stream_prefix: ecs
        auto_create: true
        features: log
```

* Assign below permission to taskExecutionRoleArn for putting log events into CloudWatch when auto\_create is not set

  ```yaml
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

* Assign below permission to taskExecutionRoleArn if auto\_create is set

  ```yaml
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:CreateLogStream",
            "logs:CreateLogGroup",
            "logs:DescribeLogStreams",
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

**CloudWatch log group name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogGroup.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogGroup.html) _\*\*_

**CloudWatch log stream name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogStream.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogStream.html) _\*\*_

### **Datadog**

If enabled, the Datadog integration will stream analytics and insights to your Datadog environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "datadog" to stream data to Datadog | Yes |
| log_host | If provided, custom installation of Datadog log host to send log data. | No |
| metric_host | If provided, custom installation of Datadog metric host to send metric data. | No |
| api\_key | Datadog API Key | Yes |
| custom\_tags | Key-values defined in custom tags by the user are streamed to datadog for every request. | No |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: datadog-integration-default
        type: datadog
        api_key: "<add datadog api key>"
        custom_tags:
          "app": "transaction_manager"
          "env": "pre_prod"
          "region": "us-west-2"

      - name: datadog-integration-custom
        type: datadog
        # If provided, custom installation of datadog log host can be reached.
        log_host: "<ADD DATADOG LOG_HOST>"
        # If provided, custom installation of datadog metric host can be reached.
        metric_host: "<ADD DATADOG METRIC_HOST>"
        api_key: "<add datadog api key>"
        features: metric
        custom_tags:
          "app": "transaction_manager"
          "region": "us-west-2"
```

**Create a new Datadog API Key:** [https://docs.datadoghq.com/account\_management/api-app-keys/\#add-a-key](https://docs.datadoghq.com/account_management/api-app-keys/#add-a-key)

### **New Relic**

If enabled, the New Relic integration will stream analytics and insights to your New Relic environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "newrelic" to stream data to New Relic | Yes |
| api\_key | New Relic Insert API Key | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: new-relic-integration
        type: newrelic
        api_key: "<add new relic insert api key>"
```

**Create a new New Relic Insert API Key:** [https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys\#event-insert-key](https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys#event-insert-key)

### **InfluxDB**

If enabled, the InfluxDB integration will stream analytics and insights to your InfluxDB deployment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "influxdb" to stream data to Influx DB | Yes |
| version | Version of the InfluxDB deployment. Supports version "1.x" and "2.x". Empty version defaults to "2.x" | No |
| bucket | Target InfluxDB bucket to send archived logs. Required only for version "2.x" | Yes |
| organization | InfluxDB buckets for customer organization. Required only for version "2.x" | Yes |
| endpoint | InfluxDB endpoint | Yes |
| http\_user | InfluxDB user credentials. Required only for version "1.x". | Yes |
| http\_password | InfluxDB password for connecting user. Required only for version "1.x". | Yes |
| db | Specific InfluxDB database to stream data to. Required only for version "1.x" | Yes |
| features | Features defines which data types stream to backend, it can be "metric", "edac". If you don't provide any value then it is all. | No |

```yaml
      - name: influxdb-integration
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        port: 443
        http_user: admin
        http_password: your_http_password
        db: "specific_influxdb_database"
      
      - name: influxdb-integration-2.x
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        token: YOUR_API_TOKEN
        # empty version or version 2.x requires bucket and organization info
        bucket: testbucket
        organization: yourorganization
        port: 443
```

### **Wavefront**

If enabled, the Wavefront integration will stream analytics and insights to your Wavefront environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "wavefront" to stream data to Wavefront | Yes |
| endpoint | Wavefront endpoint | Yes |
| token | Wavefront API token | Yes |
| features | Features defines which data types stream to backend, it can be "metric" only for Wavefront. | No |

```yaml
      - name: wavefront-integration
        type: wavefront
        endpoint: "https://{your wavefront domain}.wavefront.com/report"
        token: "<add wavefront api token>"
```

### **Scalyr**

If enabled, the Scalyr integration will stream analytics and insights to your Scalyr environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "scalyr" to stream data to Scalyr | Yes |
| endpoint | Scalyr endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log" for Scalyr. | No |

```yaml
      - name: scalyr-integration
        type: scalyr
        endpoint: "https://app.scalyr.com/api/uploadLogs?token={scalyr log access write key}"
```

### **Elastic Search**

If enabled, the Elastic Search integration will stream analytics and insights to your Elastic Search environment. Elastic index template and lifecycle creation guide can be found [here](../appendices/elastic-index.md). It's not mandatory but highly recommended to complete those steps in the guide to prepare your Elastic Search environment to be Edgedelta streaming target.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "elastic" to stream data to Elastic | Yes |
| index | Name of elastic index \(or index template\) where data will be streamed by edgedelta agents. Set this to 'ed-agent-log' if followed the guide above | Yes |
| cloud\_id | Cloud ID of elastic search backend | No |
| address | Address list of elastic search backend | No |
| token | Elastic Search API Key | No |
| user | Username for elastic search credentials | No |
| password | Elastic Search password for connecting user | No |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

* For the connection url, you can not provide cloud\_id and address at the same time. And you must provide at least one of them.
* For the authentication, you can not provide token and user/password at the same time. And you must provide at least one of them.

```yaml
      - name: elastic-integration
        type: elastic
        index: "index name"
        # you can provide cloud or adress list but not both at the same time
        cloud_id: "<add elasticsearch cloud_id>"
        #address:
         #- <elastic search endpoint address_1>
         #- <elastic search endpoint address_2>
        # you can provide token or user/pass for auth but not both at the same time
        token: "elastic search api key"
        #user: "elastic search username"
        #password: "elastic search password"
```

### Azure AppInsight

If enabled, the Azure AppInsight integration will stream analytics and insights to an Azure endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "azure" to stream data to Azure AppInsight | Yes |
| endpoint | Azure AppInsight endpoint. | Yes |
| api\_key | Azure AppInsight API key. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: azure-integration
        type: azure
        endpoint: https://dc.services.visualstudio.com/v2/track
        api_key: "Azure AppInsight api key" 
        features: "metric"
```

### Kafka

If enabled, the Kafka integration will stream analytics and insights to an Kafka endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "kafka" to stream data to Kafka | Yes |
| endpoint | Kafka broker addresses. | Yes |
| topic | Kafka topic name. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: kafka-integration
        type: kafka
        endpoint: https://dc.services.visualstudio.com/v2/track
        endpoint: <kafka broker address-1>,<kafka broker address-2>
        topic: topic
```

### SignalFx

If enabled, the SignalFx integration will stream analytics and insights to an SignalFx endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "signalfx" to stream data to SignalFX | Yes |
| endpoint | SignalFx endpoint. | Yes |
| token | SignalFx API token. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: signalfx-integration
        type: signalfx
        endpoint: https://ingest.us1.signalfx.com/v2
        token: "<add signalfx api token>"
        features: "metric,log"
```

### Humio

If enabled, the Humio integration will stream analytics and insights to an Humio endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "humio" to stream data to Humio | Yes |
| endpoint | Humio endpoint. One can use cloud one as well as self-hosted one. | Yes |
| token | Humio API token. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: humio-integration
        type: humio
        endpoint: http://localhost:8080
        token: "<add humio api token here>"
        features: "metric,log"
```

### Loggly

If enabled, the SolarWinds Loggly integration will stream analytics and insights to an Loggly endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "loggly" to stream data to Loggly | Yes |
| endpoint | Loggly endpoint. One can use cloud one as well as self-hosted one. Default one is https://logs-01.loggly.com | Yes |
| token | Loggly API token. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: loggly-integration
        type: loggly
        endpoint: https://logs-01.loggly.com
        token: "<add loggly api token here>"
        features: "metric,log"
```

### Logz.io

If enabled, the Logz.io integration will stream analytics and insights to a Logz.io endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "logzio" to stream data to Logz.io | Yes |
| endpoint | Logz.io endpoint. One can use cloud one as well as self-hosted one. | Yes |
| token | Logz.io log token. Required if want to support log stream. | No |
| metric_token | Logz.io metric token. Required if want to support metric stream. | No |
| custom\_tags | Key-values defined in custom tags by the user are streamed to Logz.io for every request. | No |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```yaml
      - name: logzio
        type: logzio
        endpoint: "https://app-eu.logz.io:8071"
        token: "<add logz.io log shipping token>"
        metric_token: "<add logz.io metric shipping token>"
        custom_tags:
          "app": "starbucks_pos_transaction_manager"
          "region": "us-west-2"
```

### Loki

You can enable this integration to stream analytics and insights to a Loki endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | This key is the user-defined name of the specific destination. This key is used for mapping this destination to a workflow. | No |
| integration\_name | This key refers to the organization-level integration created on the [Integrations page](https://admin.edgedelta.com/integrations). This key can be referred in the rest of the config via _integration\_name_ . In this case, the rest of the fields are not required to be set because the additional fields are auto-populated from the organization-level integration spec. If there are multiple instances in the same integration that need to be added to a config, then you can create a custom name for each instance via the via _name_ field. In this case, each name should be used to refer to a destination's specific instance in the workflow. | No |
| type | This key must be set to "loki" to stream data to Loki. | Yes |
| endpoint | This key is the Loki endpoint. | Yes |
| api\_key | This key is the Loki API key. | Yes |
| user | This key is the username for Loki. | Yes |
| custom\_tags | This key is the user-defined key-values that are streamed to Loki for every request. This key supports templating. | No |
| message\_template | This key customizes the message content. This key supports templating. | No |
| features | This key defines which data types to stream to the Loki backend. You can set this key to "log," "edac," and / or "cluster." | No |

#### **Message Template**

As an optional step, you can customize the message payload and custom tags that are sent to Loki destinations. 
  * Loki does not support the "-" character as key value.

Review the following avaialble template fields: 

| Field | Description |
| :--- | :--- | 
| Tag | This field is the user-defined tag that describes the environment, such as . prod\_us\_west\_2\_cluster. | 
| Host | This field is the hostname of the environment where the agent is running on. | 
| ConfigID | This field is the configuration ID of the corresponding agent. | 
| Source | This field is the source name, specifically the identifier of the source, such as docker container id or file name. | 
| SourceType | This field is the source type, such as "Docker" or "system." | 
| FileGlobPath | This field is the file global path. | 
| K8sPodName | This field is the Kubernetes pod name. | 
| K8sNamespace | This field is the Kubernetes namespace. | 
| K8sControllerKind | This field is the Kubernetes controller kind. | 
| K8sContainerName | This field is the Kubernetes container name. | 
| K8sContainerImage | This field is the Kubernetes container image. | 
| K8sControllerLogicalName | This field is the Kubernetes controller logical name. | 
| ECSCluster | This field is the ECS cluster name. | 
| ECSContainerName | This field is the ECS container name. | 
| ECSTaskVersion | This field is the ECS task version. | 
| ECSTaskFamily | This field is the ECS task family. | 
| DockerContainerName | This field is the Docker container name. | 


```yaml
      - name: loki-integration
        type: loki
        endpoint: "https://localhost:3000/loki/api/v1/push"
        api_key: "api_key"
        user: "user"
        custom_tags:
          "app": "test"
          "region": "us-west-2"
        message_template:
          "File Path": "{{.FileGlobPath}}"
          "K8s PodName": "{{.K8sPodName}}"
          "K8s Namespace": "{{.K8sNamespace}}"
          "K8s ControllerKind": "{{.K8sControllerKind}}"
          "K8s ContainerName": "{{.K8sContainerName}}"
          "K8s ContainerImage": "{{.K8sContainerImage}}"
          "K8s ControllerLogicalName": "{{.K8sControllerLogicalName}}"
          "ECSCluster": "{{.ECSCluster}}"
          "ECSContainerName": "{{.ECSContainerName}}"
          "ECSTaskVersion": "{{.ECSTaskVersion}}"
          "ECSTaskFamily": "{{.ECSTaskFamily}}"
          "DockerContainerName": "{{.DockerContainerName}}"
          "ConfigID": "{{.ConfigID}}"
          "Host": "{{.Host}}"
          "Source": "{{.Source}}"
          "SourceType": "{{.SourceType}}"
          "Tag": "{{.Tag}}"
```

### FluentD

You can enable this integration to stream analytics and insights to a FluentD endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | This key is the user-defined name of the specific destination. This key is used for mapping this destination to a workflow. | No |
| integration\_name | This key refers to the organization-level integration created on the [Integrations page](https://admin.edgedelta.com/integrations). This key can be referred in the rest of the config via _integration\_name_ . In this case, the rest of the fields are not required to be set because the additional fields are auto-populated from the organization-level integration spec. If there are multiple instances in the same integration that need to be added to a config, then you can create a custom name for each instance via the via _name_ field. In this case, each name should be used to refer to a destination's specific instance in the workflow. | No |
| type | This key must be set to "fluentd" to stream data to FluentD. | Yes |
| host | This key is the FluentD host. This key is required if you want to support tcp stream. | Yes |
| port | This key is the FluentD port. This key is required if want to support tcp stream. | Yes |
| encoder | This key is the encoder type to use while streaming data to FluentD. Raw and 'msgpack' are supported. | No |
| features | This key defines which data types to stream to the backend. You can set this key to "log", "metric," "edac," "cluster," "topk," and / or "all". If you do not provide a value, then "all" is used. | No |

```yaml
      - name: fluentd-log-fwd
        type: fluentd
        host: log-repo-host
        port: 23131
        encoder: msgpack
        pool_size: 10
        features: log
```

### Azure Event Hub Stream

You can enable this integration to stream analytics and insights to an Azure Event Hub endpoint.

> **Note**
> 
> To enable this integration, you must have an Azure AD token. To learn how to create an Azure AD token, review this [document from Microsoft](https://docs.microsoft.com/en-us/rest/api/eventhub/get-azure-active-directory-token).

| Key | Description | Required |
| :--- | :--- | :--- |
| name | This key is the user-defined name of the specific destination. This key is used for mapping this destination to a workflow. | No |
| integration\_name | This key refers to the organization-level integration created on the [Integrations page](https://admin.edgedelta.com/integrations). This key can be referred in the rest of the config via _integration\_name_ . In this case, the rest of the fields are not required to be set because the additional fields are auto-populated from the organization-level integration spec. If there are multiple instances in the same integration that need to be added to a config, then you can create a custom name for each instance via the via _name_ field. In this case, each name should be used to refer to a destination's specific instance in the workflow. | No |
| type | This key must be set to "eventhubstream" to stream data to Azure Event Hub. | Yes |
| endpoint | This key is the Event Hub endpoint. | Yes |
| token | This key is the Azure AD token. | Yes |
| features | This key defines which data types to stream to the  backend, You can set this key to "log," "metric," "edac," "cluster," "topk," "alert," and / or "all". If you do not provide a value, then "all" is used. | No |

```yaml
      - name: eventhub-stream
        type: eventhubstream
        endpoint: "https://namespace.servicebus.windows.net/hub/messages"
        token: "azure-ad-token"
        features: log,metric
```

## Trigger Destinations

### **Slack**

If enabled, the Slack integration will stream notifications and alerts to the specified Slack channel

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "slack" to send alerts to Slack | Yes |
| endpoint | Slack Webhook or APP endpoint URL | Yes |
| suppression\_window | A [golang duration](https://golang.org/pkg/time/#ParseDuration) string that represents the suppression window. Once agent detects an issue and notifies this slack endpoint it will suppress any new issues for this duration. Default is "20m". | No |
| suppression\_mode | Suppression mode can be "local" or "global". Default is "local" which means an individual agent suppresses an issue only if it has locally notified a similar issue in last suppresson window. When "global" mode is selected an individual agent checks with Edge Delta backend to see whether there were similar alerts from other sibling agents \(the ones sharing same tag in config\). | No |
| notify\_content | Used to customize the notification content. It supports templating. | No |

```yaml
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
        notify_content:
          title: "Anomaly Detected: {{.ProcessorDescription}}"
          disable_default_fields: false
          custom_fields:
            "Dashboard": "https://admin.edgedelta.com/investigation?edac={{.EDAC}}&timestamp={{.Timestamp}}"
            "Current Value": "{{.CurrentValue}}"
            "Threshold Value": "{{.ThresholdValue}}"
            "Custom Message": "{{.CurrentValue}} exceeds {{.ThresholdValue}}"
            "Matched Term": "{{.MatchedTerm}}"
```

**Getting started with Slack Incoming Webhooks:** [https://api.slack.com/messaging/webhooks](https://api.slack.com/messaging/webhooks)

### **Microsoft Teams**

If enabled, the Microsoft Teams integration will stream notifications and alerts to the specified Teams channel using hook URL

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "teams" to send alerts to Microsoft Teams | Yes |
| endpoint | Microsoft Teams Webhook URL | Yes |
| suppression\_window | A [golang duration](https://golang.org/pkg/time/#ParseDuration) string that represents the suppression window. Once agent detects an issue and notifies this Microsoft Teams endpoint it will suppress any new issues for this duration. Default is "20m". | No |
| suppression\_mode | Suppression mode can be "local" or "global". Default is "local" which means an individual agent suppresses an issue only if it has locally notified a similar issue in last suppresson window. When "global" mode is selected an individual agent checks with Edge Delta backend to see whether there were similar alerts from other sibling agents \(the ones sharing same tag in config\). | No |
| notify\_content | Used to customize the notification content. It supports templating. | No |

```yaml
      - name: microsoft-teams-integration
        type: teams
        endpoint: "https://outlook.office.com/webhookb2/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX@XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX/IncomingWebhook/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX"
        notify_content:
          title: "Anomaly Detected: {{.ProcessorDescription}}"
          disable_default_fields: false
          custom_fields:
            "Dashboard": "https://admin.edgedelta.com/investigation?edac={{.EDAC}}&timestamp={{.Timestamp}}"
            "Current Value": "{{.CurrentValue}}"
            "Threshold Value": "{{.ThresholdValue}}"
            "Custom Message": "{{.CurrentValue}} exceeds {{.ThresholdValue}}"
            "Matched Term": "{{.MatchedTerm}}"
```

#### **Notify Content**

Notify Content is optional way to customize the notification content for some (such as Slack, Microsoft Teams, Webhook etc.) triggers. It supports templating.
 **Available template fields**:

* **Tag**: User defined tag to describe the environment. e.g. prod\_us\_west\_2\_cluster.
* **EDAC**: Edge Delta Anomaly Context ID.
* **Host**: Hostname of the environment where agent running on.
* **ConfigID**: Configuration ID which agent is using.
* **MetricName**: Metric name causing the anomaly.
* **Source**: Source name is the identifier of the source such as docker container id or file name.
* **SourceType**: Source type. e.g. "Docker", "system"
* **SourceAttributes**: List of source attributes.
* **Timestamp**: Timestamp when event triggered.
* **Epoch**: Timestamp in epoch format when event triggered. [epoch](https://en.wikipedia.org/wiki/Epoch)
* **CurrentValue**: Metric current value.
* **ThresholdValue**: Threshold value.
* **ThresholdDescription**: Detailed threshold description including threshold type, value, etc.
* **MatchedTerm**: A sample log message causing the anomaly event.
* **ThresholdType**: Threshold type.
* **FileGlobPath**: File global path.
* **K8sPodName**: Kubernetes pod name.
* **K8sNamespace**: Kubernetes namespace.
* **K8sControllerKind**: Kubernetes controller kind.
* **K8sContainerName**: Kubernetes container name.
* **K8sContainerImage** Kubernetes container image.
* **K8sControllerLogicalName**: Kubernetes controller logical name.
* **ECSCluster**: ECS cluster name.
* **ECSContainerName**: ECS container name.
* **ECSTaskVersion**: ECS task version/
* **ECSTaskFamily**: ECS task family.
* **DockerContainerName**: Docker container name.

  _Note:_ About templates you should read before use:

* if the value is empty the item will not be sent to slack
* the keys are sorted alphabetically before sending to slack so they will not appear in the order specified in the config

**Title**: Title text for webhook message. It can be customized with custom template fields.
 **Disable default fields**: It is used for disabling default fields in notify message. Its value is false by default.
 **Custom Fields**: You can extend the notification content by adding name-value pairs. You can build by using template fields given above.
 **Advanced Content**: It provides full flexibility to define the payload in slack notification post requests.

* Advanced content overrides the other settings\(title, custom fields...\)
* Custom templates are also supported in advanced content.
* You can use block kit builder tool provided by Slack [https://app.slack.com/block-kit-builder](https://app.slack.com/block-kit-builder) prior to test.

```yaml
       notify_content:
         title: "Anomaly Detected: {{.ProcessorDescription}}"
         disable_default_fields: false
         custom_fields:
           "Dashboard": "https://admin.edgedelta.com/investigation?edac={{.EDAC}}&timestamp={{.Timestamp}}"
           "Current Value": "{{.CurrentValue}}"
           "Threshold Value": "{{.ThresholdValue}}"
           "Custom Message": "{{.CurrentValue}} exceeds {{.ThresholdValue}}"
           "Built-in Threshold Description": "{{.ThresholdDescription}}"
           "Matched Term": "{{.MatchedTerm}}"
           "Threshold Type": "{{.ThresholdType}}"
           "File Path": "{{.FileGlobPath}}"
           "K8s PodName": "{{.K8sPodName}}"
           "K8s Namespace": "{{.K8sNamespace}}"
           "K8s ControllerKind": "{{.K8sControllerKind}}"
           "K8s ContainerName": "{{.K8sContainerName}}"
           "K8s ContainerImage": "{{.K8sContainerImage}}"
           "K8s ControllerLogicalName": "{{.K8sControllerLogicalName}}"
           "ECSCluster": "{{.ECSCluster}}"
           "ECSContainerName": "{{.ECSContainerName}}"
           "ECSTaskVersion": "{{.ECSTaskVersion}}"
           "ECSTaskFamily": "{{.ECSTaskFamily}}"
           "DockerContainerName": "{{.DockerContainerName}}"
           "SourceAttributes": "{{.SourceAttributes}}"
           "ConfigID": "{{.ConfigID}}"
           "EDAC": "{{.EDAC}}"
           "Epoch": "{{.Epoch}}"
           "Host": "{{.Host}}"
           "MetricName": "{{.MetricName}}"
           "Source": "{{.Source}}"
           "SourceType": "{{.SourceType}}"
           "Tag": "{{.Tag}}"
```

```yaml
       notify_content:
         title: "Anomaly Detected: {{.ProcessorDescription}}"
         advanced_content: |
           {
             "blocks": [
               {
                 "type": "section",
                 "text": {
                   "type": "mrkdwn",
                   "text": "*Raw POST Anomaly Detected: {{.ProcessorDescription}}*"
                 }
               },
               {
                 "type": "section",
                 "text": {
                   "type": "mrkdwn",
                   "text": "*MatchedTerm* {{.MatchedTerm}}\n*ConfigID* {{.ConfigID}}"
                 }
               }
             ]
           }
```

### **Pagerduty**

If enabled, the Pagerduty integration will stream notifications and alerts to the specified Pagerduty API endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "pagerduty" to send alerts to Pagerduty | Yes |
| endpoint | Pagerduty API endpoint URL | Yes |
| custom\_headers | Used to append some custom headers (such as Authorization etc.) to requests done by the integration | No |
| notify\_content | Used to customize the notification content. It supports templating. It is not required but advised to use `advanced_content` subfield. | No |

```yaml
      - name: pagerduty-integration
        type: pagerduty
        endpoint: "https://api.pagerduty.com/XXXXX"
        notify_content:
          advanced_content: |
            {
              "incident": {
                "type": "incident",
                "title": "{{ .Title }}",
                "service": {
                  "id": "<ID of the pagerduty service which can be fetched via services rest API>",
                  "type": "service_reference"
                },
                "body": {
                  "type": "incident_body",
                  "details": "<Message for your incident>"
                }
              }
            }
```

### **Jira**

If enabled, the Jira integration (it makes use of CodeBarrel webhook) will stream notifications and alerts to the specified Jira webhook URL

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "jira" to send alerts to Jira | Yes |
| endpoint | Jira webhook URL | Yes |
| notify\_content | Used to customize the notification content. It supports templating. It is not required but advised to use `advanced_content` subfield. | No |

```yaml
      - name: jira-integration
        type: jira
        endpoint: "https://automation.codebarrel.io/pro/hooks/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX"
        notify_content:
          advanced_content: |
            {
              "data": {
                "title": "{{ .Title }}",
                "message": "{{ .Message }}"
              }
            }
```

### **Service Now**

If enabled, the Service Now integration will stream notifications and alerts to the specified Service Now API endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "servicenow" to send alerts to Service Now | Yes |
| endpoint | Service Now URL | Yes |
| username | Username for Service Now basic authentication | No |
| password | Password for Service Now basic authentication | No |
| custom\_headers | Used to append some custom headers (such as Authorization etc.) to requests done by the integration | No |
| notify\_content | Used to customize the notification content. It supports templating. It is not required but advised to use `advanced_content` subfield. | No |

```yaml
      - name: service-now-integration
        type: servicenow
        endpoint: "https://instance.service-now.com/api/now/table/incident"
        notify_content:
          advanced_content: |
            {
              'short_description': 'Raw POST Anomaly Detected: {{.ProcessorDescription}}',
              'assignment_group':'287ebd7da9fe198100f92cc8d1d2154e',
              'urgency':'2',
              'impact':'2'
            }
```

### **Webhook**

If enabled, the Webhook integration will stream notifications and alerts to the specified Webhook URL

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "webhook" to send alerts to custom Webhook | Yes |
| endpoint | Webhook API endpoint | Yes |
| username | Username for Webhook basic authentication | No |
| password | Password for Webhook basic authentication | No |
| notify\_content | Used to customize the notification content. It supports templating. It is not required but advised to use `advanced_content` subfield. | No |

```yaml
      - name: webhook-integration
        type: webhook
        endpoint: "localhost"
        payload:
          signature: "{{.MetricName}}"
          source_id: "{{.Host}}"
          external_id: "{{.EDAC}}"
          manager: "edgedelta"
          source: "{{.Host}}"
          class: "application"
          agent_location: "{{.Host}}"
          type: "{{.SourceType}}"
          agent_time: "{{.Epoch}}"
        notify_content:
          advanced_content: |
            {
              "foo": {
                "title": "{{ .Title }}",
                "message": "{{ .Message }}",
                "foo2": "bar2"
              }
            }
```

### **AWS Lambda**

If enabled, the AWS Lambda integration will stream notifications and alerts to the specified AWS Lambda FaaS endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "awslambda" to send alerts to AWS Lambda | Yes |
| endpoint | AWS Lambda FaaS Endpoint | Yes |
| notify\_content | Used to customize the notification content. It supports templating. It is not required but advised to use `advanced_content` subfield. | No |

```yaml
      - name: aws-lambda-integration
        type: awslambda
        endpoint: "https://XXXXXXXXXX.execute-api.XXXXXXXXX.amazonaws.com/XXXX/XXXXXX"
        notify_content:
          advanced_content: |
            {
              "foo": "bar",
              "title": "{{ .Title }}",
              "message": "{{ .Message }}"
            }
```

### **Azure Functions**

If enabled, the Azure Functions integration will stream notifications and alerts to the specified AWS Lambda FaaS endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "azurefunctions" to send alerts to Azure Functions | Yes |
| endpoint | Azure Functions FaaS Endpoint | Yes |
| notify\_content | Used to customize the notification content. It supports templating. It is not required but advised to use `advanced_content` subfield. | No |

```yaml
      - name: azure-functions-integration
        type: azurefunctions
        endpoint: "https://XXXXXXXXXX.azurewebsites.net/XXXX/XXXXXX"
        notify_content:
          advanced_content: |
            {
              "foo": "bar",
              "title": "{{ .Title }}",
              "message": "{{ .Message }}"
            }
```

### Examples

```yaml
outputs:
  streams:
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
  triggers:
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
```

## Archive Destinations

### **AWS S3**

If enabled, the AWS S3 integration will send logs to an AWS S3 endpoint.

Before you configure your Edge Delta account to sends logs to an AWS S3 endpoint, you must first access the AWS console to attach the following custom policy to the IAM user that will have access to the AWS S3 bucket: 

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Action":[
            "s3:PutObject"
         ],
         "Effect":"Allow",
         "Resource":[
            "arn:aws:s3:::your_bucketname/*",
            "arn:aws:s3:::your_bucketname"
         ]
      }
   ]
}
```

After you attach the policy in the AWS console, review the following parameters that you can configure in the Edge Delta Admin portal:

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "s3" to send archived logs to AWS S3 | Yes |
| bucket | Target s3 bucket to send archived logs | Yes |
| region | The specified s3 bucket's region | Yes |
| aws\_key\_id | AWS key id that has PutObject permission to target bucket. How do I create an AWS access key? [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key). If you use role-based AWS authentication where keys are not provided, then you should keep this field empty. | No |
| aws\_sec\_key | AWS secret key id that has PutObject permission to target bucket. How do I create an AWS access key? [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key). If you use role-based AWS authentication where keys are not provided, then you should keep this field empty.  | No |

```yaml
      - name: my-s3
        type: s3
        bucket: testbucket
        region: us-east-2
        aws_key_id: "<add aws key id>"
        aws_sec_key: "<add aws secure key>"
```

### **Azure Blob Storage**

If enabled, the Azure Blob Storage integration will stream logs to an Azure Blob Storage endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "blob" to send archived logs to Azure blob | Yes |
| account\_name | Account Name for the azure account. | Yes |
| account\_key | Account Key for azure account. You can visit [https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) | Yes |
| container | Container to upload. | Yes |
| auto_create_container | Create the container on the service (with no metadata and no public access). | No |

```yaml
      - name: my-blob
        type: blob
        account_name: "<add account name>"
        account_key: "<add account key>"
        container: testcontainer
```

### **Google Cloud Storage**

If enabled, the Google Cloud Storage integration will stream logs to an GCS endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "gcs" to send archived logs to Google Cloud Storage | Yes |
| bucket | Target gcs bucket to send archived logs. | Yes |
| hmac\_access\_key | GCS HMAC Access Key which has permissions to upload files to specified bucket. See [https://cloud.google.com/storage/docs/authentication/managing-hmackeys](https://cloud.google.com/storage/docs/authentication/managing-hmackeys) for details on how to create new keys | Yes |
| hmac\_secret | GCS HMAC secret associated with the access key specified. | Yes |

```yaml
      - name: my-gcs
        type: gcs
        bucket: ed-test-bucket
        hmac_access_key: my_hmac_access_key_123
        hmac_secret: my_hmac_secret_123
```

### **DigitalOcean Spaces**

If enabled, the DigitalOcean Spaces integration will stream logs to an DigitalOcean Spaces endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "dos" to send archived logs to DigitalOcean Spaces | Yes |
| endpoint | DigitalOcean Spaces Endpoint | Yes |
| bucket | Target DOS bucket to send archived logs. | Yes |
| access\_key | Access Key which has permissions to upload files to specified bucket. | Yes |
| secret\_key | Secret Key associated with the access key specified. | Yes |

```yaml
      - name: my-digitalocean-spaces
        type: dos
        endpoint: nyc3.digitaloceanspaces.com
        bucket: ed-test-bucket
        access_key: my_access_key_123
        secret_key: my_secret_key_123
```

### **IBM Object Storage**

If enabled, the IBM Object Storage integration will stream logs to an IBM Object Storage endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "ibmos" to send archived logs to IBM Object Storage | Yes |
| endpoint | IBM Object Storage Endpoint | Yes |
| bucket | Target IBM OS bucket to send archived logs. | Yes |
| access\_key | Access Key which has permissions to upload files to specified bucket. | Yes |
| secret\_key | Secret Key associated with the access key specified. | Yes |

```yaml
      - name: my-ibm-object-storage
        type: ibmos
        endpoint: s3-api.us-geo.objectstorage.softlayer.net
        bucket: ed-test-bucket
        access_key: my_access_key_123
        secret_key: my_secret_key_123
```

### **Minio**

If enabled, the Minio integration will stream logs to an Minio endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "minio" to send archived logs to Minio | Yes |
| endpoint | Minio Endpoint | Yes |
| bucket | Target Minio bucket to send archived logs. | Yes |
| access\_key | Access Key which has permissions to upload files to specified bucket. | Yes |
| secret\_key | Secret Key associated with the access key specified. | Yes |
| disable\_ssl | Disable SSL requirement when pushing logs to Minio endpoint | No |
| s3\_force\_path\_style | Force archive destination to use `{endpoint}/{bucket}` format instead of `{bucket}.{endpoint}/` when reaching buckets`) | No |

```yaml
      - name: my-minio
        type: minio
        endpoint: play.min.io:9000
        bucket: ed-test-bucket
        access_key: my_access_key_123
        secret_key: my_secret_key_123
        disable_ssl: true
        s3_force_path_style: true
```

### **Zenko CloudServer**

If enabled, the Zenko CloudServer integration will stream logs to an CloudServer endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows. | No |
| type | Must be set to "zenko" to send archived logs to Zenko | Yes |
| endpoint | Zenko Endpoint | Yes |
| bucket | Target Zenko bucket to send archived logs. | Yes |
| access\_key | Access Key which has permissions to upload files to specified bucket. | Yes |
| secret\_key | Secret Key associated with the access key specified. | Yes |

```yaml
      - name: my-zenko-cloudserver
        type: zenko
        endpoint: https://XXXXXXXXXX.sandbox.zenko.io
        bucket: ed-test-bucket
        access_key: my_access_key_123
        secret_key: my_secret_key_123
```

### **Moogsoft**

If enabled, the Moogsoft integration will stream notifications and alerts to the specified Moogsoft URL

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "moogsoft" to send alerts to Moogsoft | Yes |
| endpoint | Moogsoft API endpoint | Yes |
| api_key | Moogsoft API Key. One of the fields API Key or username/password is required. | No |
| username | Username for Moogsoft basic authentication. One of the fields username/password or API Key is required. | No |
| password | Password for Moogsoft basic authentication. One of the fields username/password or API Key is required. | No |
| notify\_content | Used to customize the notification content. It supports templating. Moogsoft only supports `custom_fields` subfield. | No |

```yaml
      - name: moogsoft-default
        type: moogsoft
        endpoint: "https://example.moogsoftaiops.com/events/webhook_webhook1"
        api_key: "moogsoft-apikey"
        notify_content:
          custom_fields:
            "jira-ticket": "ticket"
```

### **Remedy**

If enabled, the Remedy integration will stream notifications and alerts to the specified Remedy URL.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | No |
| integration\_name | Integration name refers to the organization level integration created on [Integrations page](https://admin.edgedelta.com/integrations). It can be referred in the rest of the config via _integration\_name_ in which case rest of the fields are not required to be set because rest is auto-populated from org level integration spec. In case multiple instances of same integration needs to be added to a config then a custom name can be given to each via _name_ field. In that case name should be used to refer the specific instance of the destination in workflows.  | No |
| type | Must be set to "remedy" to send alerts to Remedy | Yes |
| endpoint | Remedy API endpoint | Yes |
| token | Remedy token. One of the fields token or username/password is required. | No |
| username | Username for Remedy basic authentication. One of the fields username/password or token is required. | No |
| password | Password for Remedy basic authentication. One of the fields username/password or token is required. | No |
| custom\_headers | Used to append some custom headers (such as Authorization etc.) to requests done by the integration | No |
| notify\_content | Used to customize the notification content. It supports templating. Moogsoft only supports `custom_fields` subfield. | No |

```yaml
      - name: remedy-default
        type: remedy
        endpoint: "localhost"
        token: remedy-token
        notify_content:
          custom_fields:
            "test-field": "test"
        custom_headers: 
          X-header1: "test-header"
```

### **Azure Event Hub Trigger**

You can enable this integration to stream notifications and alerts to a specified Event Hub URL. 

> **Note**
> 
> To enable this integration, you must have an Azure AD token. To learn how to create an Azure AD token, review this [document from Microsoft](https://docs.microsoft.com/en-us/rest/api/eventhub/get-azure-active-directory-token).


| Key | Description | Required |
| :--- | :--- | :--- |
| name | This key is the user-defined name of the specific destination. This key is used for mapping this destination to a workflow. | No |
| integration\_name | This key refers to the organization-level integration created on the [Integrations page](https://admin.edgedelta.com/integrations). This key can be referred in the rest of the config via _integration\_name_ . In this case, the rest of the fields are not required to be set because the additional fields are auto-populated from the organization-level integration spec. If there are multiple instances in the same integration that need to be added to a config, then you can create a custom name for each instance via the via _name_ field. In this case, each name should be used to refer to a destination's specific instance in the workflow. | No |
| type | This key must be set to "eventhub" to send alerts to Event Hub. | Yes |
| endpoint | This key is the Event Hub endpoint. | Yes |
| token | This key is the Azure AD token. | Yes |
| custom\_headers | This key appends custom headers, such as Authorization, to requests performed by the integration. | No |
| notify\_content | This key customizes the notification content. This key supports templating. Event Hub only supports the `custom_fields` subfield. | No |

```yaml
       - name: eventhub-test
        type: eventhub
        endpoint: https://eventshub-test.servicebus.windows.net/test/messages
        token: "test-token"
        notify_content:
          custom_fields:
            "test-field": "test"
        custom_headers: 
          X-header1: "test-header"
```
