---
description: >-
  This document focuses on streaming outputs.  
---

# Outputs - Streams 

## Overview

You can use this document to learn about the configuration parameters available in a configuration file, specifically for **Outputs - Streams**.

An **Output - Stream** focuses on centralized monitoring platforms. Specificailly, this output type tells the Edge Delta agent which centralized monitoring platform to send collected and generated data. 

> **Note**
> 
> Edge Delta offers additional output types, specifically **Trigger** and **Archive**. 
> To learn more, see XXXXXXX and XXXXXX

<br>

> **Note**
> 
> The terms **output**, **integration**, and **destination** may be used interchangeably. 

***

## Step 1: Review Feature Types

In the Edge Delta Admin portal, **features** are the data types that the Edge Delta agent should collect (or generate), and then send to a streaming destination. 

When you create an **Output - Stream**, you can add the following features to the output: 

| Feature Type | Description | 
| :--- | :--- | 
| metric | This feaure sends metrics that are generated by processors in the workflow. By default, this feature type is enabled. | 
| edac | This feature sends contextual logs that happened around an anomaly. **edac** represents Edge Delta Anomaly Context. By default, this feature type is enabled.  | 
| cluster | This feature sends cluster patterns and samples. Patterns are sent in the following format: "{cluster-pattern}, {count}" By default, this feature type is enabled.  | 
| logs | This feaure forwards raw logs to a streaming destination. | 
| topk | This feaure sends top-k records that are generated by the top-k processor. | 
| all | This feature enables the **metric**, **edac**, and **cluster** features for streaming destinations. | 

***

## Step 2: Create or Update Outputs - Streams

At a high level, there are 2 ways to manage **Outputs - Streams**:

  * If you need to create a new configuration, then you can use the visual editor to populate a YAML file, as well as make changes directly in the YAML file.
  * If you already have an existing configuration, then you can update the configuration in the YAML file. 

***

### Option 1: Access the visual editor for a new configuration

1. In the Edge Delta Admin portal, on the left-side navigation, click **Agent Settings**.
2. Click **Create Configuration**.
3. Click **Visual**.
4. On the right-side, select **Streams**.
5. Select the desired destination, and then complete the missing fields. 

  * To learn more about each destination, specifically parameters, see [Step 3: Review Parameters for Streaming Destinations](#step-3-review-parameters-for-streaming-destinations).

6. To make additional configurations to the configuration file, click the back button, and then select a new configuration section to manage. 
7. To save the configuration and exit the visual editor, click **Save**. 
8. Refresh the screen to view the newly created configuration in the table. 

***

### Option 2: Access the YAML file for an existing configuration

1. In the Edge Delta Admin portal, on the left-side navigation, click **Agent Settings**.
2. Locate the desired configuration, and then under **Actions**, click the corresponding edit icon.
3. Review the YAML file, make your changes, and then click **Save**. 

  * To learn more about each destination, specifically parameters, see [Step 3: Review Parameters for Streaming Destinations](#step-3-review-parameters-for-streaming-destinations).

***

## Step 3: Review Parameters for Streaming Destinations

Edge Delta supports the following streaming destinations: 

### Splunk

The Splunk output will stream analytics and insights to a Splunk HEC endpoint. 

> **Before you begin**
> 
> To create an output, you must have available a Splunk HEC token and HEC endpont.
> To learn how to create and obtain this information, see [Set Up the Splunk Integration](#set-up-the-splunk-integration).

In the Edge Delta Admin portal, in the visual editor, when you select **Splunk** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **splunk**. | Required |
| endpoint | Enter the full Splunk HEC URI for this integration. | Required |
| token | Enter the Splunk HEC token for this integration. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **metric,edac,cluster** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: splunk-integration
        type: splunk
        endpoint: "<protocol>://<host>:<port>/<endpoint>"
        token: "32-character GUID token"
        index: "main"
```

The following example displays if enter the name of the organization-level integration:

```yaml
      - integration_name: my-org-splunk
```

The following example displays if there are multiple instances of the same desitnation that need to route different data types to different Splunk indexes: 

```yaml
- name: edac-splunk-dest
  integration_name: orgs-splunk
  features: edac
  index: edac-index
- integration_name: orgs-splunk
  name: metric-splunk-dest
  features: metric
  index: metric-index
```

***

#### Set Up the Splunk Integration

Before you can set up a Splunk output, you must have the HEC token and HEC endpoint avaialble. At a high level, to set up a Splunk output, you must: 

* Configure an HEC token in Splunk
* Determine the correct HEC endpoint in Splunk
* Import the Edge Delta dashboard into Splunk

> **Note**
> 
> The process to set up a Splunk output varies for Splunk Cloud and Splunk Enterprise users.


#### Step 1: Configure an HEC Token in Splunk

##### Option 1: Splunk Cloud
* Create a Splunk HTTP Event Collector (HEC) and Token
* In the Splunk Web UI, navigate to Settings -> Add Data
* Click “Monitor” -> “HTTP Event Listener”
* Provide a name for the HEC in the name field -> Click “Next”
* Confirm Index information or use default index -> “Click Review”
* Click “Submit”
* Copy the Token Value displayed in the Splunk Web into the “Token” field of the Edge Delta - Splunk Streaming Destination Configuration

##### Option 2: Splunk Enterprise

* Ensure HTTP Event Collector (HEC) is enabled
* In the Splunk Enterprise Web UI, navigate to Settings -> Data Inputs
* Click “HTTP Event Collector”
* Click “Global Settings”
* In the “All Tokens” toggle button, select “Enabled”
* Create a Splunk HTTP Event Collector (HEC) and Token
* In the Splunk Web UI, navigate to Settings -> Add Data
* Click “Monitor” -> “HTTP Event Collector”
* Provide a name for the HEC in the name field -> Click “Next”
* Confirm Index information or use default index -> “Click Review”
* Click “Submit”
* Copy the Token Value displayed in the Splunk Web into the “Token” field of the Edge Delta - Splunk Streaming Destination Configuration


#### Step 2: Determine your HEC Endpoint

Before you continue, verify that you have the following information: 

* Splunk deployment type (Enterprise, Cloud, Free Trial, etc)
* Splunk hostname (from Splunk Browser URI) 
* Input  Protocol (HTTPS is default)

##### Option 1: Splunk Cloud (Cloud, Free Trial, Cloud on GCP) Format

Replace <splunk_hostname> with your organization’s hostname

* Splunk Cloud
  * URI Format: https://http-inputs-<splunk_hostname>:443/services/collector/event
* Splunk Free Trial 
  * URI Format: https://inputs.<splunk_hostname>:8088/services/collector/event
* Splunk Cloud on GCP
  * URI Format: https://http-inputs.<splunk_hostname>:443/services/collector/event

##### Option 2: Splunk Enterprise

* URI Format: https://<splunk_hostname>:8088/services/collector/event


#### Step 3: Import the Edge Delta Dashboard

Review the following instructions to learn how to import your Edge Delta dashboard into your Splunk environment: 

* In Splunk, navigate to Search UI.
* Click **Dashboards**.
* Click **Create New Dashboard**.
* Input Dashboard Name/Description/Permissions
* Click “Classic Dashboards” -> “Create”
* In the Edit Dashboard Screen, switch from UI to Source
* Replace existing XML with copied XML from clipboard
* Switch back to UI (instead of source)
* Click **Save**.

  
***

### Sumo Logic

The Sumo Logic output will stream analytics and insights to a Sumo Logic HTTPs Endpoint.

> **Before you begin**
> 
> Before you can create an output, you must have available the Sumo Logic HTTPs Endpoint.
> To learn how to create new Sumo Logic HTTPs endpoint, review this [document from Sumo Logic](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url). 
> To learn how to locate an existing Sumo Logic HTTPs endpoint, review this [document from Sumo Logic](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url).

In the Edge Delta Admin portal, in the visual editor, when you select **Sumo Logic** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional|
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **sumologic**. | Required |
| endpoint | Enter the full HTTPs URL for this endpoint. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:


```yaml
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
```

***

### AWS CloudWatch

The AWS CloudWatch output will stream logs to a specified AWS region.  

> **Before you begin**
> 
> Before you can create an output, you must have available the CloudWatch log group name and log stream name.
> To learn how to create a log group, review this [document from Amazon](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogGroup.html).
> To learn how to create a log stream, review this [document from Amazon](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogStream.html).

In the Edge Delta Admin portal, in the visual editor, when you select **AWS CloudWatch** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional|
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **cloudwatch**. | Required |
| region | Enter the AWS region destination to send logs. | Required |
| log\_group\_name | Enter the CloudWatch log group name. | Required |
| log\_stream\_name | Enter the CloudWatch log stream name. You can enter a name or prefix, but not both. | Required |
| log\_stream\_prefix | Enter the CloudWatch log stream prefix. You can enter a name or prefix, but not both.  | Required |
| auto\_create | When necessary iam policies provided if auto\_create is set, log group and log stream will be created if not exists | Optional |
| allow\_label\_override | monitored container can override the default values of log group name, logs stream name and log stream prefix, by setting ed\_log\_group\_name, ed\_log\_stream\_name, ed\_log\_stream\_prefix labels | Optional |
| auto\_configure | only supported for ECS environments, and when provided only region configuration can be provided. Automatically create LogGroupName in the format of /ecs/task\_definition\_family and LogsStreamPrefix in the format of ecs/container\_name/task\_id | Optional |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Required |
| features | This parameter defines which data types to stream to the backend. For Amazon CloudWatch, you can only select **log**. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: cloudwatch
        type: cloudwatch
        region: us-west-2
        log_group_name: /ecs/microservice
        log_stream_prefix: ecs
        auto_create: true
        features: log
```

* Assign below permission to taskExecutionRoleArn for putting log events into CloudWatch when auto\_create is not set

  ```yaml
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

* Assign below permission to taskExecutionRoleArn if auto\_create is set

  ```yaml
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:CreateLogStream",
            "logs:CreateLogGroup",
            "logs:DescribeLogStreams",
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

***


### Datadog

The Datadog output will stream analytics and insights to a Datadog environment. 

> **Before you begin**
> 
> Before you can create an output, you must have available a Datadog API Key.
> To learn how to create a new Datadog API key, review this [document from Datadog](https://docs.datadoghq.com/account_management/api-app-keys/#add-a-key).

In the Edge Delta Admin portal, in the visual editor, when you select **Datadog** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **datadog**. | Required |
| log_host | If provided, custom installation of Datadog log host to send log data. | Optional |
| metric_host | If provided, custom installation of Datadog metric host to send metric data. | Optional |
| api\_key | Enter a Datadog API key. | Yes |
| custom\_tags | Key-values defined in custom tags by the user are streamed to datadog for every request. | Optional |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |


The following example displays an output without the name of the organization-level integration:

```yaml
      - name: datadog-integration-default
        type: datadog
        api_key: "<add datadog api key>"
        custom_tags:
          "app": "transaction_manager"
          "env": "pre_prod"
          "region": "us-west-2"

      - name: datadog-integration-custom
        type: datadog
        # If provided, custom installation of datadog log host can be reached.
        log_host: "<ADD DATADOG LOG_HOST>"
        # If provided, custom installation of datadog metric host can be reached.
        metric_host: "<ADD DATADOG METRIC_HOST>"
        api_key: "<add datadog api key>"
        features: metric
        custom_tags:
          "app": "transaction_manager"
          "region": "us-west-2"
```

***

### New Relic

The New Relic output will stream analytics and insights to a New Relic environment.

> **Before you begin**
> 
> Before you can create an output, you must have available the New Relic Insert API key. 
> To learn how to create new New Relic Insert API key, review this [document from New Relic](https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys#event-insert-key).

In the Edge Delta Admin portal, in the visual editor, when you select **New Relic** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **newrelic**. | Required |
| api\_key | Enter a New Relic Insert API key. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set.  | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: new-relic-integration
        type: newrelic
        api_key: "<add new relic insert api key>"
```

***

### InfluxDB

The InfluxDB output will stream analytics and insights to your InfluxDB deployment.

In the Edge Delta Admin portal, in the visual editor, when you select **InfluxDB** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **influxdb**. | Required |
| version | Enter the version number of the InfluxDB deployment. This parameter supports versions 1.x and 2.x. An empty version will default to version 2.x | Optional |
| bucket | Enter the InfluxDB bucket to send archived logs to. This parameter is only required for version 2.x. | Required |
| organization | Enter the InfluxDB buckets for customer organization. This parameter is only required for version 2.x. | Required |
| endpoint | Enter the InfluxDB endpoint. | Required |
| http\_user | Enter the InfluxDB user credentials. This parameter is only required for version 1.x. | Required |
| http\_password | Enter the InfluxDB password for connecting user. This parameter is only required for version 1.x. | Required |
| db | Enter the InfluxDB database to stream data to. This parameter is only required for version 1.x. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: influxdb-integration
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        port: 443
        http_user: admin
        http_password: your_http_password
        db: "specific_influxdb_database"
      
      - name: influxdb-integration-2.x
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        token: YOUR_API_TOKEN
        # empty version or version 2.x requires bucket and organization info
        bucket: testbucket
        organization: yourorganization
        port: 443
```

***

### Wavefront

The Wavefront output will stream analytics and insights to your Wavefront environment.

In the Edge Delta Admin portal, in the visual editor, when you select **Wavefront** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow.  | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **wavefront**. | Required |
| endpoint | Enter the Wavefront endpoint. | Required |
| token | Enter the Wavefront API token. | Required |
| features | This parameter defines which data types to stream to the backend. For Wavefront, you can only select **metric**. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: wavefront-integration
        type: wavefront
        endpoint: "https://{your wavefront domain}.wavefront.com/report"
        token: "<add wavefront api token>"
```

***

### Scalyr

The Scalyr output will stream analytics and insights to your Scalyr environment.

In the Edge Delta Admin portal, in the visual editor, when you select **Scalyr** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **scalyr**.| Required |
| endpoint | Enter the Scalyr endpoint. | Required |
| features | This parameter defines which data types to stream to the backend. For Scalyr, you can only select **log**. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: scalyr-integration
        type: scalyr
        endpoint: "https://app.scalyr.com/api/uploadLogs?token={scalyr log access write key}"
```

***

### Elastic Search

The **Elastic Search** output will stream analytics and insights to your Elastic Search environment. 

> **Note**
> 
> Edge Delta recommends that you review and complete the steps listed in the [elastic index template and lifecycle creation guide](../appendices/elastic-index.md). This process will help you prepare your Elastic Search environment to become an Edge Detla streaming target.

> Note
> 
> For **connection url**, you must provide either the **cloud\_id** or **address**. You cannot enter both parameters. 
> For the **authentication**, you must provide either the **token** or the **user/password**. You cannot enter both parameters. 

In the Edge Delta Admin portal, in the visual editor, when you select **Elastic Search** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **elastic**. | Required |
| index | Enter the name of the Elastic index \(or index template\) where Edge Delta should stream the data. If you followed the guide mentioned above, then set this parameter to **ed-agent-log**. | Required |
| cloud\_id | Enter the cloud ID of the Eastic Search backend. | Optional |
| address | Enter the address list of the Eastic Search backend. | Optional |
| token | Enter the Eastic Search API key. | Optional |
| user | Enter the username of the Elastic Search credentials. | Optional |
| password | Enter the password for the connecting user. | Optional |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: elastic-integration
        type: elastic
        index: "index name"
        # you can provide cloud or adress list but not both at the same time
        cloud_id: "<add elasticsearch cloud_id>"
        #address:
         #- <elastic search endpoint address_1>
         #- <elastic search endpoint address_2>
        # you can provide token or user/pass for auth but not both at the same time
        token: "elastic search api key"
        #user: "elastic search username"
        #password: "elastic search password"
```

***

### Azure AppInsight

The **Azure AppInsight** output will stream analytics and insights to your Azure endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **Azure AppInsight** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **azure**. | Required |
| endpoint | Enter the Azure AppInsight endpoint. | Required |
| api\_key | Enter the Azure AppInsight API key. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: azure-integration
        type: azure
        endpoint: https://dc.services.visualstudio.com/v2/track
        api_key: "Azure AppInsight api key" 
        features: "metric"
```

***

### Kafka

The **Kafka** output will stream analytics and insights to your Kafka endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **Kafka** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **kafka**. | Required |
| endpoint | Enter your Kafka broker addresses. | Required |
| topic | Enter your Kafka topic name. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: kafka-integration
        type: kafka
        endpoint: https://dc.services.visualstudio.com/v2/track
        endpoint: <kafka broker address-1>,<kafka broker address-2>
        topic: topic
```

***

### SignalFx

The **SignalFx** output will stream analytics and insights to your SignalFx endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **SignalFx** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow.  | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **signalfx**. | Required |
| endpoint | Enter your SignalFx endpoint. | Required |
| token | Enter your SignalFx API token. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: signalfx-integration
        type: signalfx
        endpoint: https://ingest.us1.signalfx.com/v2
        token: "<add signalfx api token>"
        features: "metric,log"
```

***

### Humio

The **Humio** output will stream analytics and insights to your Humio endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **Humio** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **humio**. | Required |
| endpoint | Enter the Humio endpoint. You can use a cloud endpoint or a self-hosted endpoint. | Required |
| token | Enter the Humio API token. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: humio-integration
        type: humio
        endpoint: http://localhost:8080
        token: "<add humio api token here>"
        features: "metric,log"
```

***

### Loggly

The **Loggly** output will stream analytics and insights to your Loggly endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **Loggly** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional|
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **loggly**. | Required |
| endpoint | Enter a Loggly endpoint. You can use a cloud endpoint or a self-hosted endpoint. The default endpoint is https://logs-01.loggly.com. | Required |
| token | Enter a Loggly API token. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: loggly-integration
        type: loggly
        endpoint: https://logs-01.loggly.com
        token: "<add loggly api token here>"
        features: "metric,log"
```

***

### Logz.io

The **Logz.io** output will stream analytics and insights to your Logz.io endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **Logz.io** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **logzio**. | Required |
| endpoint | Enter the Logz.io endpoint. You can use a cloud endpoint or a self-hosted endpoint. | Required |
| token | Enter the Logz.io log token. This parameter is required if you want to support log stream. | Optional |
| metric_token | Enter the Logz.io metric token. This parameter is required if you want to support metric stream. | Optional |
| custom\_tags | With this parameter, you can create key-values to stream to Logz.io with every request. | Optional |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: logzio
        type: logzio
        endpoint: "https://app-eu.logz.io:8071"
        token: "<add logz.io log shipping token>"
        metric_token: "<add logz.io metric shipping token>"
        custom_tags:
          "app": "starbucks_pos_transaction_manager"
          "region": "us-west-2"
```

***

### Loki

The **Loki** output will stream analytics and insights to your Loki endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **Loki** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **loki**. | Required |
| endpoint | Enter the Loki endpoint. | Required |
| api\_key | Enter the Loki API key.  | Required |
| user | Enter the username for Loki. | Required |
| custom\_tags | With this parameter, you can create key-values to stream to Loki with every request. This parameter supports templating. | Optional |
| message\_template | This parameters customizes the message content. This parameter supports templating. | Optional |
| features | This parameter defines which data types to stream to the backend. You can select **log**, **edac**, and / or **cluster**. | Optional |

#### **Message Template**

As an optional step, you can customize the message payload and custom tags that are sent to Loki destinations. 
  * Loki does not support the **-** character as a key value.

Review the following template fields: 

| Field | Description |
| :--- | :--- | 
| Tag | This field is the user-defined tag that describes the environment, such as **prod\_us\_west\_2\_cluster**. | 
| Host | This field is the hostname of the environment where the agent is running on. | 
| ConfigID | This field is the configuration ID of the corresponding agent. | 
| Source | This field is the source name, specifically the identifier of the source, such as docker container id or file name. | 
| SourceType | This field is the source type, such as **Docker** or **system**. | 
| FileGlobPath | This field is the file global path. | 
| K8sPodName | This field is the Kubernetes pod name. | 
| K8sNamespace | This field is the Kubernetes namespace. | 
| K8sControllerKind | This field is the Kubernetes controller kind. | 
| K8sContainerName | This field is the Kubernetes container name. | 
| K8sContainerImage | This field is the Kubernetes container image. | 
| K8sControllerLogicalName | This field is the Kubernetes controller logical name. | 
| ECSCluster | This field is the ECS cluster name. | 
| ECSContainerName | This field is the ECS container name. | 
| ECSTaskVersion | This field is the ECS task version. | 
| ECSTaskFamily | This field is the ECS task family. | 
| DockerContainerName | This field is the Docker container name. | 

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: loki-integration
        type: loki
        endpoint: "https://localhost:3000/loki/api/v1/push"
        api_key: "api_key"
        user: "user"
        custom_tags:
          "app": "test"
          "region": "us-west-2"
        message_template:
          "File Path": "{{.FileGlobPath}}"
          "K8s PodName": "{{.K8sPodName}}"
          "K8s Namespace": "{{.K8sNamespace}}"
          "K8s ControllerKind": "{{.K8sControllerKind}}"
          "K8s ContainerName": "{{.K8sContainerName}}"
          "K8s ContainerImage": "{{.K8sContainerImage}}"
          "K8s ControllerLogicalName": "{{.K8sControllerLogicalName}}"
          "ECSCluster": "{{.ECSCluster}}"
          "ECSContainerName": "{{.ECSContainerName}}"
          "ECSTaskVersion": "{{.ECSTaskVersion}}"
          "ECSTaskFamily": "{{.ECSTaskFamily}}"
          "DockerContainerName": "{{.DockerContainerName}}"
          "ConfigID": "{{.ConfigID}}"
          "Host": "{{.Host}}"
          "Source": "{{.Source}}"
          "SourceType": "{{.SourceType}}"
          "Tag": "{{.Tag}}"
```

***

### FluentD

The **FluentD** output will stream analytics and insights to your FluentD endpoint.

In the Edge Delta Admin portal, in the visual editor, when you select **FluentD** as the output type, the following fields will appear:

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow. | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows. | Optional |
| type | You must set this parameter to **fluentd**. | Required |
| host | Enter the FluentD host. This parametetr is required to support tcp stream. | Required |
| port | Enter the FluentD port. This parameter is required to support tcp stream. | Required |
| encoder | Enter the encoder type to use while streaming data to FluentD. Raw and 'msgpack' are supported. | Optional |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set.  | Optional |

The following example displays an output without the name of the organization-level integration:


```yaml
      - name: fluentd-log-fwd
        type: fluentd
        host: log-repo-host
        port: 23131
        encoder: msgpack
        pool_size: 10
        features: log
```

***

### Azure Event Hub Stream

The **Azure Event Hub Stream** output will stream analytics and insights to your Azure Event Hub Stream endpoint.

> **Note**
> 
> This output is not avaialble in the visual editor. To configure, you must make changes directly in the YAML file.  

<br>

> **Note**
> 
> To enable this integration, you must have an Azure AD token. To learn how to create an Azure AD token, review this [document from Microsoft](https://docs.microsoft.com/en-us/rest/api/eventhub/get-azure-active-directory-token).

| Parameter | Description | Required or Optional |
| :--- | :--- | :--- |
| name | Enter a descriptive name for the output, which will be used to map this destination to a workflow.  | Optional |
| integration\_name | This parameter refers to the organization-level integration created in the **Integrations** page. If you enter this name, then the rest of the fields will be automatically populated. If you need to add multiple instances of the same integration into the config, then you can add a custom name to each instance via the **name** field. In this situation, the name should be used to refer to the specific instance of the destination in the workflows.  | Optional |
| type | You must set this parameter to **eventhubstream**. | Required |
| endpoint | Enter the Event Hub endpoint. | Required |
| token | Enter the Azure AD token. | Required |
| features | This parameter defines which data types to stream to the backend. If you do not provide a value, then **all** will be set. | Optional |

The following example displays an output without the name of the organization-level integration:

```yaml
      - name: eventhub-stream
        type: eventhubstream
        endpoint: "https://namespace.servicebus.windows.net/hub/messages"
        token: "azure-ad-token"
        features: log,metric
```

***


